# Анализ обновлений библиотек и выбор GNN слоев (2025-2026)

Этот документ суммирует результаты исследования обновлений библиотек `NeuralForecast` и `PyTorch Geometric` (PyG) и уточняет выбор архитектуры графовых слоев для задачи прогнозирования дебита.

## 1. Статус библиотек

### NeuralForecast (v3.1.4)
*   **Статус:** Релиз от 15 января 2026 (v3.1.4).
*   **Важные изменения:**
    *   **v3.0.0 (Breaking Changes):** Все модели теперь наследуют `BaseModel`. Унифицирован API для квантилей и вероятностных прогнозов.
    *   **TimeXer:** Новая модель, добавленная в v3.0.0, которая может быть интересна в будущем, но TSMixerx остается стабильным выбором.
    *   **Улучшения:** Поддержка `bfloat16`, оптимизация памяти в `tsdataset`, улучшенная работа со статическими экзогенными переменными (`stat_exog`).
*   **Вывод:** Наша стратегия интеграции через `hist_exog` полностью валидна. Необходимо учитывать изменения в API v3.0.0 при наследовании (обязательный `input_size` для рекуррентных моделей).

### PyTorch Geometric (v2.7.0)
*   **Статус:** Релиз от 14 октября 2025 (v2.7.0).
*   **Новое и важное:**
    *   **Поддержка PyTorch 2.4+:** Полная совместимость с новыми версиями torch.
    *   **Index Tensor:** Новый класс `Index` для эффективного хранения 1D индексов, ускоряющий работу с разреженными данными.
    *   **LLM Integration:** Новый подпакет `torch_geometric.nn.nlp` и модель `GRetriever` (GAT + LLM), что подтверждает тренд на гибридные архитектуры.
    *   **Explainability:** Значительные улучшения в `torch_geometric.explain` (HeteroExplanation, новые алгоритмы важности фичей). Это поможет нам объяснять бизнесу, почему модель выбрала тех или иных соседей.

## 2. Выбор Графового Слоя (SOTA 2025)

На основе анализа документации и Cheatsheet:

### Кандидаты:

1.  **GATv2Conv (Graph Attention Network v2):**
    *   **Статус:** Поддерживается, является стандартом для динамического внимания.
    *   **Плюсы:** Исправляет проблему статического внимания обычного GAT. Позволяет модели динамически решать, какой сосед важен *прямо сейчас*.
    *   **Вердикт:** Это **золотой стандарт** для нашей задачи (Spatial-Geological Perception). Он идеально подходит для реализации SGP-GCN.

2.  **TransformerConv:**
    *   **Статус:** Доступен.
    *   **Суть:** Использует полноценный механизм Multi-Head Attention, аналогичный трансформерам в NLP.
    *   **Сравнение с GATv2:** Может быть мощнее на больших графах, но GATv2Conv обычно достаточно для задач с ~50-100 узлами. Можно рассмотреть как альтернативу при тюнинге.

3.  **GPSConv (General Powerful Scalable Graph Transformers):**
    *   **Статус:** Доступен (добавлен в v2.2+).
    *   **Суть:** Гибрид MPNN (Message Passing) и Transformer. Очень мощный слой.
    *   **Вердикт:** Возможно избыточен сейчас, но стоит иметь в виду как "тяжелую артиллерию".

## 3. Уточненная архитектура (Heterogeneous GNN)

Учитывая возможности PyG 2.7, мы можем сделать граф **гетерогенным** (разные типы узлов и связей), а не просто мульти-графом на матрицах.

*   **Типы узлов:** `Producer` (добывающие), `Injector` (нагнетательные).
*   **Типы связей:**
    *   `Producer` <-> `Producer` (Geo distance): GATv2Conv (взаимное влияние, интерференция).
    *   `Injector` -> `Producer` (Water injection): GATv2Conv (влияние закачки, однонаправленное).

Это позволит модели выучить *разные* физические законы для интерференции ("сосед забрал мою нефть") и заводнения ("нагнеталка выдавила нефть ко мне").

## 4. Итоговое решение

1.  **Библиотеки:** Используем последние версии (`neuralforecast>=3.1.4`, `torch_geometric>=2.7.0`).
2.  **Слой:** **GATv2Conv** остается лучшим выбором. Он мощнее обычного GCN (учитывает важность) и стабильнее сложных трансформеров на малых графах.
3.  **Структура:** Рекомендуется явно использовать `HeteroData` из PyG 2.7 для разделения нагнетательных и добывающих скважин, если это не усложнит код чрезмерно. Для начала можно остаться на гомогенном графе с маской типов скважин (как фича узла), что проще в реализации.

**В код вносим `GATv2Conv`.**
